{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a27a46-5684-4f07-9930-8976a46e473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef650f8-56a2-43ec-b08f-6a287b99d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The K-Nearest Neighbors (KNN) algorithm is a supervised learning algorithm used for classification and regression tasks.\n",
    "\n",
    "# Stores training data\n",
    "# Calculates distances between new data points and training data\n",
    "# Finds the K closest neighbors\n",
    "# Assigns the most common class (classification) or average value (regression) to the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30f507f-2b82-4f6b-8278-57621e0bbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0e603e-364d-4b2e-9803-0c4d9793d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation: Test different ùêæ values and choose the one with the best performance.\n",
    "# Grid Search: Use GridSearchCV to find the best ùêæ from a predefined range.\n",
    "# Elbow Method: Plot the error metric vs. ùêæ and select the ùêæ at the \"elbow\" point.\n",
    "# Odd Values: Prefer odd numbers to avoid ties in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6b76a5-ad6f-4dfb-84ea-4c13bad25adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57e27a4-581f-491b-a2ff-e6e7425c2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier:\n",
    "# Predicts a class label (e.g., 0 or 1, yes or no, spam or not spam) based on the majority vote of the K nearest neighbors.\n",
    "# Used for classification problems (e.g., image classification, sentiment analysis).\n",
    "# Output is a category or class.\n",
    "\n",
    "# KNN Regressor:\n",
    "# Predicts a continuous value (e.g., a number, a price, a temperature) based on the average of the K nearest neighbors' values.\n",
    "# Used for regression problems (e.g., predicting house prices, stock prices).\n",
    "# Output is a number or value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9337eab3-7321-4357-bfad-5ea4554c21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76a1dd3-9f3b-484f-9d38-58562fb86fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onfusion Matrix: \n",
    "#     A table that summarizes the performance of a classification model. It includes the number of true \n",
    "# positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "# Accuracy: \n",
    "#     The percentage of correct predictions out of all predictions. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "    \n",
    "# Recall (Sensitivity): \n",
    "#     The percentage of true positives among all actual positives. It is calculated as TP / (TP + FN).\n",
    "    \n",
    "# F1 Score: \n",
    "#     A harmonic mean of precision and recall. It is calculated as 2 * (Precision * Recall) / (Precision + Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8d35aa-9ec6-4736-bdb8-766e05cac2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e243accb-c9ed-470a-a6c7-d0f7f7263c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The curse of dimensionality in KNN refers to the phenomenon where, as the number of features (dimensions) increases, the\n",
    "# distance between data points becomes less meaningful. This makes it difficult for KNN to find truly similar neighbors, \n",
    "# leading to poor performance and increased computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e768032-fe46-43a9-804a-34b1ce83d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14279e31-abb1-4db5-912e-526327ebaea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean/Median Imputation: \n",
    "#Replace missing values with the mean or median of the respective feature.\n",
    "\n",
    "# K-Nearest Neighbors Imputation: \n",
    "#Use a KNN algorithm to impute missing values based on the similarity between samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e917dd02-cf2e-4cfa-8e37-312b75da482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "# which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a194a2-d058-44e6-a720-7086a499caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier:\n",
    "# Purpose: Classifies categorical data.\n",
    "# Output: Determines the class label based on the majority vote of K nearest neighbors.\n",
    "# Performance: Effective with clear class boundaries; sensitive to noise and irrelevant features.\n",
    "# Metrics: Accuracy, precision, recall, F1-score.\n",
    "\n",
    "# KNN Regressor:\n",
    "# Purpose: Predicts continuous values.\n",
    "# Output: Calculates the average value of K nearest neighbors.\n",
    "# Performance: Suitable for data with trends or patterns; can be affected by noise and high-dimensionality.\n",
    "# Metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared.\n",
    "\n",
    "\n",
    "# KNN Classifier is better suited for classification tasks with discrete outcomes.\n",
    "# KNN Regressor is better suited for regression tasks with continuous outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f297dbd-013a-422d-9470-55b1fa61c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "# and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486fe950-7b48-4458-9be2-d028f0f035ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strengths of KNN:\n",
    "# Simple to implement: Easy to understand and code\n",
    "# Flexible: Handles categorical and numerical features, and can be used for both classification and regression tasks\n",
    "# Robust to noisy data: Can handle noisy or missing data\n",
    "# Non-parametric: Doesn't require a specific distribution of the data\n",
    "\n",
    "# Weaknesses of KNN:\n",
    "# Computationally expensive: Can be slow for large datasets\n",
    "# Sensitive to feature scaling: Features with large ranges can dominate the distance metric\n",
    "# Sensitive to choice of distance metric: Different metrics can lead to different results\n",
    "# Curse of dimensionality: Performance degrades with high-dimensional data\n",
    "\n",
    "# Addressing the weaknesses:\n",
    "# Use efficient algorithms: Implement optimized algorithms, such as ball trees or k-d trees, to reduce computational cost\n",
    "# Feature scaling: Normalize or standardize features to prevent dominance\n",
    "# Choose the right distance metric: Select a metric suitable for the problem, such as Euclidean, Manhattan, or Minkowski\n",
    "# Dimensionality reduction: Apply techniques, such as PCA or feature selection, to reduce the number of features\n",
    "# Use weighted KNN: Assign weights to neighbors based on their distance to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe71f36-d427-4c86-aa9c-a3bec88f49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc81110-8638-4e0e-8fb0-27c4d87566e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance:\n",
    "# Also known as L2 distance\n",
    "# Calculates the straight-line distance between two points\n",
    "# Formula: ‚àö(‚àë(xi - yi)^2)\n",
    "# Sensitive to outliers and noisy data\n",
    "# More accurate for datasets with continuous features\n",
    "\n",
    "# Manhattan Distance:\n",
    "# Also known as L1 distance\n",
    "# Calculates the sum of the absolute differences between two points\n",
    "# Formula: ‚àë|xi - yi|\n",
    "# More robust to outliers and noisy data\n",
    "# More accurate for datasets with categorical or ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b9825e9-e9c0-44b4-ac83-22d71acebee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39bfea-12bc-4280-8d6b-3fa3dfb4d90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
