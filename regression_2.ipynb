{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2796941-fc6b-4dec-8804-f51ca01839e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "# represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74cc427b-9725-43a2-a22e-82e8748b04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared in Linear Regression\n",
    "#     Concept: R-squared (R²) measures how much of the variation in the dependent variable can be explained by the independent\n",
    "#     variables in the model.\n",
    "\n",
    "# Calculation:\n",
    "#     R-squared is calculated as:\n",
    "#         1 minus the ratio of the sum of squared residuals (the differences between the observed and predicted values) to the\n",
    "#         total sum of squares (the differences between the observed values and the mean of the observed values).\n",
    "        \n",
    "# Representation:\n",
    "#     Range: R-squared ranges from 0 to 1.\n",
    "#         0: Indicates that the model explains none of the variance in the dependent variable.\n",
    "#         1: Indicates that the model explains all of the variance in the dependent variable.\n",
    "#         Closer to 1: The closer R-squared is to 1, the better the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84e6e281-16ec-4c68-82bc-272d48c4482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f3b12c0-3198-4c74-9497-daa30a34b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition: Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the\n",
    "# model. It accounts for the complexity of the model, providing a more accurate measure of how well the independent variables \n",
    "# explain the variance in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e879efd-d53e-49da-9fb4-9645347c38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences: Adjusted R-squared penalizes unnecessary predictors, discouraging overfitting, while R-squared does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23f43bb2-c201-43e3-8df0-a44e616461ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fb9115-a971-41c1-a291-d295f18b0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adjusted R-squared: \n",
    "#     When you have multiple predictors, need to compare models with different complexities, want to prevent overfitting, or\n",
    "#     are dealing with complex models.\n",
    "    \n",
    "# Why: It adjusts for the number of predictors, offering a more accurate measure of model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a9c3e35-646c-48b7-99b3-d45ce46f84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "# calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7054be64-1098-44ae-9fd6-8cf1c251e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE: Measures the average absolute error. Less sensitive to outliers.\n",
    "# MSE: Measures the average squared error. More sensitive to large errors.\n",
    "# RMSE: The square root of MSE. Provides error in the same units as the target variable.\n",
    "\n",
    "# Use Cases:\n",
    "# MAE: Preferred when all errors are equally important.\n",
    "# MSE: Used when larger errors are more significant and should be penalized more.\n",
    "# RMSE: Useful for interpreting model performance in the same units as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a06fc311-b9b7-4ce5-9161-3d0714f8bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "# regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28227dca-c79e-462d-a1c6-4db7b8ba0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE:\n",
    "#     Pros: Easy to interpret, robust to outliers.\n",
    "#     Cons: Does not penalize larger errors heavily.\n",
    "    \n",
    "# MSE:\n",
    "#     Pros: Penalizes large errors, smooth derivatives for optimization.\n",
    "#     Cons: Sensitive to outliers, less interpretable due to squared units.\n",
    "    \n",
    "# RMSE:\n",
    "#     Pros: Interpretable in the same unit as the target variable, penalizes large errors.\n",
    "#     Cons: Sensitive to outliers, more complex to calculate than MAE.\n",
    "    \n",
    "# Use Cases\n",
    "#     MAE: When you want a straightforward measure of average error and need robustness against outliers.\n",
    "#     MSE: When larger errors need to be penalized more, and you're using gradient-based optimization.\n",
    "#     RMSE: When you want the interpretability of the metric in the same unit as the target and need to penalize larger errors more than MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4a2c18f-91b7-40be-b899-a80996b149ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "# it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3aa34190-0dc7-43c3-8f55-fd92e4e74062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regularization:\n",
    "#     Adds L1 penalty (absolute values).\n",
    "#     Can shrink some coefficients to zero, leading to feature selection.\n",
    "#     Use when you need a simpler model with fewer predictors.\n",
    "    \n",
    "# Ridge Regularization:\n",
    "#     Adds L2 penalty (squared values).\n",
    "#     Shrinks coefficients but keeps all predictors.\n",
    "#     Use when dealing with multicollinearity and when all features are believed to be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88e0e82e-e5e9-4caa-82d8-8b0c1998c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "# example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85ea65d3-4ab1-42a5-8902-f9cd8cbf6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9633012a-0a28-4e61-a5f3-04764a6ceca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1500, 3], [1600, 3], [1700, 3], [1800, 4], [1900, 4], [2000, 4], [2100, 5]])\n",
    "y = np.array([300000, 320000, 340000, 360000, 380000, 400000, 420000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a1f0e2c-0b1a-44aa-823f-760bd490d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7c65d07-bdbb-48ed-851d-f9c9497da335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6698d17e-86ac-4bde-80bb-65191d1a7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "test_error = mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba5cbcf0-d75e-4117-9534-ee2bf1a0fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error (No Regularization): 1.1293772630057337e-21\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Error (No Regularization): {test_error}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a8e342b-7035-4c69-a9bc-d1699dd07864",
   "metadata": {},
   "source": [
    "With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "333c1d47-1423-438b-ba14-85845bfda17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc8270a7-ebf3-47f9-bd74-db4f21edc4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model = Ridge(alpha=1.0) \n",
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bc1274a-9bbd-40df-8314-035927aaf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ridge = ridge_model.predict(X_test)\n",
    "test_error_ridge = mean_squared_error(y_test, y_pred_test_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd723c12-9293-4157-b0e1-05fc87e0b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error (Ridge): 1.0746941791861084\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Error (Ridge): {test_error_ridge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "129733e7-c698-49f6-8217-2a95f37ccdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "# choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3aa7bb1-a006-47f7-bc64-77623f5daa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Complexity: \n",
    "#     Regularized linear models may struggle with complex, non-linear relationships and interactions.\n",
    "#     Feature Selection: Lasso may exclude important features, especially in the presence of correlated features.\n",
    "    \n",
    "# Handling Non-Linear Relationships: \n",
    "#     They assume linear relationships, which may not be suitable for all datasets.\n",
    "    \n",
    "# Sensitivity to Hyperparameters:\n",
    "#     Performance relies on selecting the correct regularization strength.\n",
    "        \n",
    "# Interpretability:\n",
    "#     Regularization can complicate the interpretation of model coefficients.\n",
    "#     Performance with Small Datasets: Regularization might introduce excessive bias in small datasets.\n",
    "    \n",
    "#     When to Consider Alternatives:\n",
    "#         For capturing non-linear patterns, consider using non-linear models like decision trees, random forests, or neural\n",
    "#         networks.\n",
    "#         For feature selection in high-dimensional datasets with correlated features, techniques like Elastic Net or \n",
    "#         advanced feature selection methods might be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ae75c64-edb3-4766-86be-d47313f9b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "# Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "# performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c969523-1c1f-46b3-b051-3b21683ca223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison:\n",
    "#     Model A: RMSE of 10 (sensitive to large errors and outliers).\n",
    "#     Model B: MAE of 8 (less sensitive to outliers and gives a clear average error).\n",
    "    \n",
    "# Choosing the Better Model:\n",
    "#     Model B might be better if you want to minimize the average error and deal with fewer large errors.\n",
    "#     Model A might be better if large errors need to be penalized more heavily.\n",
    "    \n",
    "# Limitations:\n",
    "#     RMSE: Can be skewed by outliers and might not reflect typical performance well.\n",
    "#     MAE: Provides a straightforward average error but doesn’t account for the impact of larger errors.\n",
    "    \n",
    "# Metric Selection:\n",
    "#     Choose based on what matters more: average error (MAE) or penalizing large errors (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa8bb7a1-d310-48d3-9e6d-01cb5a2ef1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "# regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "# uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "# better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "# method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44de78ea-4f5a-4486-9f6d-51ab96672371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Lasso Regularization (Model B) if you need feature selection and want a simpler model with fewer features.\n",
    "\n",
    "# Choose Ridge Regularization (Model A) if you want to handle multicollinearity and retain all features.\n",
    "\n",
    "# Trade-Offs: Ridge regularization handles multicollinearity but doesn’t simplify the model, while Lasso regularization\n",
    "# simplifies the model but might exclude relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ace4db-f30c-4088-ba67-b25b1528af2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c5330-173b-492f-91f1-02684e16a665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676be55c-02a4-4543-aae0-7c0c002f5fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcba5e-6548-49b0-bbb8-247d30df2930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
