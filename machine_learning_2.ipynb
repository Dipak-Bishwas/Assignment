{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59907dfb-c536-4d83-a5ab-0b3f992f8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "# can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41155eee-dde1-4b16-90ec-1520f52d4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting:\n",
    "#     Definition: When a model learns the training data too well, including noise and details, leading\n",
    "#     to poor performance on new, unseen data.\n",
    "#     Consequences: Poor generalization, high variance, and inaccurate predictions on new data.\n",
    "#     Mitigation:\n",
    "#         Use more training data.\n",
    "#         Simplify the model (fewer parameters).\n",
    "#         Use regularization techniques (e.g., L1, L2).\n",
    "#         Employ cross-validation.\n",
    "        \n",
    "# Underfitting:\n",
    "    \n",
    "# Definition: When a model is too simple to capture the underlying patterns in the data, leading to poor performance\n",
    "# on both training and new data.\n",
    "#     Consequences: Poor model performance, high bias, and inaccurate predictions.\n",
    "#     Mitigation:\n",
    "#         Increase model complexity.\n",
    "#         Use more relevant features.\n",
    "#         Reduce noise in the data.\n",
    "#         Ensure sufficient training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316ffd30-bf39-4e69-bc14-71f0b03061b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac76e97e-55fd-4391-a411-c4701471b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfitting\n",
    "#     Definition: Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the\n",
    "#     data, resulting in poor performance on both the training data and new, unseen data.\n",
    "\n",
    "#     Scenarios Where Underfitting Can Occur\n",
    "#         Using a Linear Model for Non-Linear Data:\n",
    "#             Example: Applying linear regression to data that follows a complex, non-linear relationship.\n",
    "            \n",
    "#     Insufficient Model Complexity:\n",
    "#         Example: Using a shallow decision tree for a problem that requires deeper trees to capture intricate patterns.\n",
    "        \n",
    "#     Insufficient Training:\n",
    "#         Example: Training a neural network for too few epochs, leading to an underdeveloped model.\n",
    "        \n",
    "#     Over-regularization:\n",
    "#         Example: Applying too strong regularization (e.g., high L2 penalty), which excessively penalizes model \n",
    "#         complexity.\n",
    "                \n",
    "# Simple Example\n",
    "# Imagine trying to fit a straight line (linear model) to data points that form a curve (non-linear relationship).\n",
    "# The line will not capture the curve's shape, resulting in inaccurate predictions both for the training data and new\n",
    "# data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36851eb-c0ae-43ef-942e-d5fbaa7c6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "# variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f055ed49-6928-47fa-91ee-bba7f2e3d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "# Bias:\n",
    "#     Definition: Error from a model that is too simple.\n",
    "#     Effect: Underfitting, poor performance on training and test data.\n",
    "    \n",
    "# Variance:\n",
    "#     Definition: Error from a model that is too complex.\n",
    "# Effect: Overfitting, good performance on training data but poor on test data.\n",
    "\n",
    "# Relationship\n",
    "#     High Bias, Low Variance: Underfitting.\n",
    "#     Low Bias, High Variance: Overfitting.\n",
    "#     Optimal Balance: Minimizes total error, good generalization.\n",
    "    \n",
    "# Impact on Model Performance\n",
    "#     High Bias: Poor performance overall.\n",
    "#     High Variance: Poor performance on new data.\n",
    "#     Balanced: Good performance on both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabe61fc-afde-45c6-8ffc-6c1f2410d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "# How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a4bc9c-0901-43fc-8ae7-2cfc5396c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Overfitting and Underfitting\n",
    "#     Training vs. Test Performance:\n",
    "#         Overfitting: High accuracy on training data but low accuracy on test data.\n",
    "#         Underfitting: Low accuracy on both training and test data.\n",
    "        \n",
    "# Learning Curves:\n",
    "#     Overfitting: Training accuracy continues to improve while validation accuracy plateaus or worsens.\n",
    "#     Underfitting: Both training and validation accuracy are low and converge slowly.\n",
    "    \n",
    "# Cross-Validation:\n",
    "#     Overfitting: High variance in performance across different folds.\n",
    "#     Underfitting: Consistently poor performance across all folds.\n",
    "    \n",
    "# Model Complexity:\n",
    "# Overfitting: Complex models with many parameters.\n",
    "# Underfitting: Simple models with few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778e3898-3207-4c67-bd17-f0cb18d8a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "# and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7ecba4-e16a-4a7f-952c-e17c34c7f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias vs. Variance\n",
    "#     Bias:\n",
    "#         Definition: Error from a model that is too simple.\n",
    "#         Example: Linear regression for non-linear data.\n",
    "#         Performance: Poor on both training and test data (underfitting).\n",
    "        \n",
    "#     Variance:\n",
    "#         Definition: Error from a model that is too complex.\n",
    "#         Example: Deep neural network with small dataset.\n",
    "#         Performance: Good on training data but poor on test data (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85a2ce-2725-47ac-92e5-0fa494e04de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059e55d0-5b84-4cbf-976a-f951344737e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in Machine Learning\n",
    "#     Definition: Regularization helps prevent overfitting by adding a penalty for model complexity.\n",
    "\n",
    "# Common Techniques\n",
    "#     L1 Regularization (Lasso):\n",
    "#         Penalty: Absolute values of coefficients.\n",
    "#         Effect: Can reduce some coefficients to zero.\n",
    "        \n",
    "# L2 Regularization (Ridge):\n",
    "#     Penalty: Squared values of coefficients.\n",
    "#     Effect: Shrinks coefficients, reducing complexity.\n",
    "    \n",
    "# Elastic Net:\n",
    "#     Penalty: Combination of L1 and L2 penalties.\n",
    "#     Effect: Balances feature selection and coefficient shrinkage.\n",
    "    \n",
    "# Dropout:\n",
    "#     Penalty: Randomly drops neurons during training.\n",
    "#     Effect: Prevents over-reliance on specific neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879959b8-a980-4709-90e2-fa7143b9497c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
