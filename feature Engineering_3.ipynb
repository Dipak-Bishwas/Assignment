{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee291e2-9501-4147-ae7b-e4fc2dcea7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "# application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a05e42-b49a-42b2-a416-d55782955a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling, also known as normalization, is a preprocessing technique used to rescale feature values to a specific\n",
    "# range, typically [0, 1]. This method is important when features in a dataset have different scales, as it ensures all \n",
    "# features contribute equally to machine learning algorithms, particularly those relying on distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68be5b00-f3d2-46bc-98f8-c9cbc8db6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# For a feature Age with values ranging from 20 to 60, Min-Max scaling transforms these values into a range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dae739e-8f0b-442c-8b4f-ffcacd5ec955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application:\n",
    "# Min-Max scaling is implemented in Python using sklearn's MinMaxScaler. This transformation is commonly applied before \n",
    "# training machine learning models to improve their performance and ensure faster convergence.\n",
    "\n",
    "# Overall, Min-Max scaling helps standardize features, making them suitable for algorithms that are sensitive to feature \n",
    "# scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b384ac-0b32-47a5-afa4-8dcfdc210311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "# Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c910b2-aaa8-439c-bb4e-f68646a89c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Vector Scaling normalizes each feature vector to have a magnitude of 1, preserving the direction of the vector.\n",
    "\n",
    "# Min-Max Scaling rescales individual features to a specific range, focusing on the spread of feature values rather than \n",
    "# their overall magnitude.\n",
    "\n",
    "# Application: Unit Vector scaling is useful when the directionality of data points is important, such as in certain text \n",
    "# mining and clustering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece6129e-363c-47f6-976a-0fa48fc8917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "# example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c8870d-20c8-4f34-8e48-bb85adf4c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is a technique for reducing the dimensionality of a dataset by transforming it into a set of uncorrelated variables called principal components.\n",
    "\n",
    "# How It Works: PCA identifies the directions (principal components) that capture the most variance in the data and projects \n",
    "# the data onto these components.\n",
    "\n",
    "# Application: PCA is used to reduce the number of features in a dataset while retaining as much variance (information) as\n",
    "# possible, which is helpful in reducing computation time, simplifying models, and visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5097ecee-0f21-486d-b4a6-befb4224e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f8065d-a94c-45af-96dd-12b3b06a7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[2.5, 2.4, 3.0, 4.0],\n",
    "                 [0.5, 0.7, 1.0, 2.0],\n",
    "                 [2.2, 2.9, 2.8, 3.9],\n",
    "                 [1.9, 2.2, 3.1, 3.7],\n",
    "                 [3.1, 3.0, 3.4, 4.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8289aec-0921-4bbb-99cc-491ff69601e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb251ea-0100-4012-9869-8b6cbe55f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db23c977-37a5-4e1a-a48f-4b914c530006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (5, 4)\n",
      "PCA Transformed Data Shape: (5, 2)\n",
      "Explained Variance Ratio: [0.96067338 0.01999825]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"PCA Transformed Data Shape:\", data_pca.shape)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd3a7cf-0970-4ad1-a068-224419626d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "# Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadfdb7e-0839-44a3-b826-d2b647dc80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA as Feature Extraction: PCA transforms the original features into new features (principal components) that are linear \n",
    "# combinations of the original features, capturing the directions of maximum variance.\n",
    "\n",
    "# Feature Extraction: PCA reduces the dimensionality of the data while retaining the most important information, making it\n",
    "# easier to analyze and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e6a02e2-de97-4bb7-ab14-e5ddba03b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7661b83b-af75-4939-9457-3cdefefcb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac866129-6583-4d4c-8c33-a23998362df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "data_pca = pca.fit_transform(data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55200a4e-c495-4589-a64c-1289ac56f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (100, 10)\n",
      "PCA Transformed Data Shape: (100, 3)\n",
      "Explained Variance Ratio: [0.16607787 0.13284476 0.12507021]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"PCA Transformed Data Shape:\", data_pca.shape)\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "148e5870-f7f1-40b7-8c7b-63612590e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "# preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffece744-63c7-4b93-a729-bece169faefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling is a simple yet effective preprocessing technique used to normalize features such as price, rating, \n",
    "# and delivery time in a recommendation system for a food delivery service. By scaling these features to a [0, 1] range, \n",
    "# you ensure that they contribute uniformly to the model, improving its performance and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8574f22a-bc95-4708-ac20-958212a8fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'price': [5, 10, 15, 20, 25],\n",
    "    'rating': [2, 3, 5, 4, 5],\n",
    "    'delivery_time': [30, 20, 40, 50, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72fdd59b-4593-4ffa-89b2-17ca0cc3b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdcb7288-625e-4458-bc34-00c0aa154f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c9935a2-4dc2-4199-9e43-2c9a832f2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1eed5aa-2181-4270-a5ac-680381a1993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Data:\n",
      "   price    rating  delivery_time\n",
      "0   0.00  0.000000       0.333333\n",
      "1   0.25  0.333333       0.000000\n",
      "2   0.50  1.000000       0.666667\n",
      "3   0.75  0.666667       1.000000\n",
      "4   1.00  1.000000       0.166667\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScaled Data:\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af67d2b4-3c6c-4a34-a0bb-1e0c101b7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "# features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "# dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75a7eb38-d4ae-4430-9402-01f15ab6df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is a powerful technique to reduce the dimensionality of a dataset when building a stock price prediction model. \n",
    "# By transforming the original features into a smaller set of principal components, you can retain most of the dataâ€™s \n",
    "# variance while simplifying the model, leading to more efficient and potentially more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "955ea709-76c3-4aef-9d73-59dbd2bddd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [5, 4, 3, 2, 1],\n",
    "    'feature3': [2, 3, 4, 5, 6],\n",
    "    'feature4': [7, 8, 9, 10, 11]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da4bfb81-44f2-453e-9ede-1b5fadc0f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "624ae3f5-483c-4ee5-8956-64ad45153f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e622a08-dd8a-4fb7-864b-21dcd2794cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64fee31f-cf8e-406a-8e16-54b43ff6f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Data:\n",
      "[[ 2.82842712e+00  3.64856517e-16]\n",
      " [ 1.41421356e+00 -1.21618839e-16]\n",
      " [-0.00000000e+00  0.00000000e+00]\n",
      " [-1.41421356e+00  1.21618839e-16]\n",
      " [-2.82842712e+00  2.43237678e-16]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduced Data:\")\n",
    "print(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42432aa9-9dc9-4c65-bd7f-36f35ffa629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f165f41-d248-4916-aff8-0f844227546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ff6a957-2c2c-4837-889d-052e9648985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(data, new_min=-1, new_max=1):\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    scaled_data = new_min + (data - min_val) * (new_max - new_min) / (max_val - min_val)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d0f1779-2c98-46b8-8410-35d722682009",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = min_max_scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc6d52d2-f3e0-48eb-b2bd-f02841e6e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaled Data:\", scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2278d020-1b4f-4c3a-a7a6-21f9eb3b7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc235ee-8ed0-4b51-93d5-1d367d9abd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
