{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46407192-5a94-4cdb-9671-99fef3e590b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1da7cc0-94ce-4ebf-a90c-f54ea75f20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Random Forest Regressor is an ensemble method for regression that builds multiple decision trees using bootstrap \n",
    "# samples of the data. It averages the predictions from all trees to improve accuracy and robustness. It reduces overfitting \n",
    "# and handles complex relationships by introducing diversity among trees through random feature selection and aggregation \n",
    "# of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71d43dc4-cd49-4df0-8366-8e56ee799162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d887336-ce67-4ade-85cc-8ba5aeca8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning: By combining the predictions of multiple decision trees, it averages out the errors of individual\n",
    "# trees, which reduces the model's variance and overfitting.\n",
    "\n",
    "# Bootstrap Sampling: Each decision tree is trained on a different bootstrap sample (random subset with replacement) of the\n",
    "# data, which ensures that the trees are diverse and less likely to overfit to the original training data.\n",
    "\n",
    "# Feature Randomization: During the training of each tree, only a random subset of features is considered for splitting at\n",
    "# each node. This prevents any single feature from dominating the decision-making process and helps in creating diverse trees, which further reduces overfitting.\n",
    "\n",
    "# Bagging: This technique (bootstrap aggregating) helps in stabilizing the model by averaging the predictions of the multiple \n",
    "# trees, leading to a more generalized model that performs better on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b190d4-aedb-4d4d-8c0e-fdde1035f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c469d86f-f77c-41fb-8536-2780a44519f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple Trees: The algorithm builds several decision trees using different bootstrap samples of the training \n",
    "# data. Each tree makes its own prediction for a given input.\n",
    "\n",
    "# Collect Predictions: For a new data point, each decision tree in the forest produces a prediction \n",
    "# (a numerical value in the case of regression).\n",
    "\n",
    "# Average Predictions: The final prediction of the Random Forest Regressor is obtained by calculating the average of the \n",
    "# predictions from all the individual decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3276360b-eaf9-4514-bc44-a21def8d04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42a50359-0b25-474d-a7ed-495a1146b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: Number of trees in the forest.\n",
    "# max_depth: Maximum depth of each tree.\n",
    "# min_samples_split: Minimum samples required to split a node.\n",
    "# min_samples_leaf: Minimum samples required at a leaf node.\n",
    "# max_features: Number of features to consider for splitting.\n",
    "# bootstrap: Whether to use bootstrap samples.\n",
    "# oob_score: Use out-of-bag samples for validation.\n",
    "# n_jobs: Number of parallel jobs for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3246b026-777a-4373-8c71-84cf9f06a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d0535e2-cc68-429e-94f6-027d3d330fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor is a single tree model prone to overfitting and high variance, but is easier to interpret.\n",
    "# Random Forest Regressor uses an ensemble of decision trees to reduce overfitting and variance, providing better \n",
    "# performance and robustness, but is more complex to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98032ff8-7cab-4a8e-b24f-abe27fd00bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c09956cf-3393-4f21-946a-04f568f6980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Random Forest Regressor:\n",
    "# Reduced Overfitting: Averages predictions from multiple trees to avoid overfitting.\n",
    "# High Accuracy: Generally provides strong performance and robustness.\n",
    "# Handles Non-linearity: Models complex, non-linear relationships well.\n",
    "# Feature Importance: Offers insights into which features are most important.\n",
    "# Robust to Noise: Less affected by noisy data.\n",
    "\n",
    "# Disadvantages of Random Forest Regressor:\n",
    "# Complexity: Harder to interpret compared to single decision trees.\n",
    "# Computationally Intensive: Requires more resources and can be slower.\n",
    "# Model Size: Can become large and cumbersome.\n",
    "# Less Interpretability: The ensemble approach reduces transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faf0a0b3-0484-477c-aa66-8091eb50e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c1716ac-98f9-4e10-a2d6-82c25399d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of a Random Forest Regressor is a numerical value representing the predicted value for a given input.\n",
    "\n",
    "# How It Works:\n",
    "# Individual Predictions: Each decision tree in the forest makes a prediction based on the input features.\n",
    "# Aggregation: The Random Forest Regressor averages the predictions from all the individual decision trees to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "749ea137-bc99-4457-a830-dcec1142a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec2dac52-4291-4c0f-90e2-1241636dfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# No, the Random Forest Regressor is specifically designed for regression tasks, where the goal is to predict continuous\n",
    "# numerical values. For classification tasks, you would use the Random Forest Classifier.\n",
    "\n",
    "# Key Differences:\n",
    "# Random Forest Regressor: Outputs a continuous value by averaging the predictions of multiple decision trees.\n",
    "# Random Forest Classifier: Outputs a class label by using majority voting from multiple decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e085921-0e34-4ca2-9f45-916a1323a2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
