{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a24ec8c-df59-4d42-9bc3-14b4a49e9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d1248b-57ee-4ceb-878f-637b31dab455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PCA, projection is the process of mapping high-dimensional data onto a lower-dimensional space defined by principal\n",
    "# components. This involves:\n",
    "\n",
    "# Identifying Principal Components: Determining the directions (axes) that capture the most variance in the data.\n",
    "\n",
    "# Transforming Data: Projecting the original data onto these new axes, thereby reducing dimensionality while preserving the \n",
    "# most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156f389e-c262-4499-82c6-1f54c4f7cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf38056-b1bf-436a-94b4-5039175a9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PCA, the optimization problem aims to find directions (principal components) that maximize the variance of the data.\n",
    "# The process involves:\n",
    "    \n",
    "# Objective: Maximize variance in the reduced-dimensional space.\n",
    "\n",
    "# Steps:\n",
    "# Compute the covariance matrix of the data.\n",
    "# Solve for eigenvectors (principal components) and eigenvalues (variances) of the covariance matrix.\n",
    "# Sort eigenvectors by eigenvalues in descending order.\n",
    "# Project the data onto the top ùëò eigenvectors for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83983894-6708-41f7-a154-46ec12a7bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e333e7f-74ac-429e-a9c1-c96c95c8fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance Matrix: It captures how features vary with respect to each other. Specifically, it shows the variance along\n",
    "# each feature and the covariance between pairs of features.\n",
    "\n",
    "# PCA Process:\n",
    "# Compute Covariance Matrix: This matrix is derived from the data to understand the relationships between features.\n",
    "\n",
    "# Eigen Decomposition: PCA performs eigen decomposition on the covariance matrix to find eigenvalues and eigenvectors.\n",
    "\n",
    "# Principal Components: The eigenvectors represent the directions (principal components) of maximum variance. The eigenvalues \n",
    "# indicate the magnitude of variance along these directions.\n",
    "\n",
    "# Projection: The data is projected onto the principal components (eigenvectors) to achieve dimensionality reduction while\n",
    "# preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b02eac2-d75a-4d6d-a8f7-09ba6554c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaae5663-9b78-4501-a6ce-fc54a138a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the number of principal components in PCA impacts model performance by balancing data reduction with information \n",
    "# retention. Fewer components simplify the model and improve efficiency but may lose important information. More components \n",
    "# capture more variance but can lead to overfitting and increased computational costs. The goal is to retain enough variance\n",
    "# to maintain model performance while simplifying the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbae9207-3439-4f60-b7b0-5e38a1bfcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31829477-e243-4d2b-922a-2e3d6ecc80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA for Feature Selection:\n",
    "\n",
    "# Dimensionality Reduction: PCA reduces the number of features by projecting data onto a lower-dimensional space. This helps\n",
    "# in selecting the most important features that explain the most variance in the data.\n",
    "\n",
    "# Feature Importance: By analyzing the principal components, you can identify which original features contribute most to the\n",
    "# variance in the data. Features with higher loadings in the first few principal components are considered more important.\n",
    "\n",
    "# Benefits:\n",
    "# Improved Model Performance: Reducing the number of features can decrease overfitting and improve model generalization.\n",
    "# Enhanced Computational Efficiency: Fewer features result in faster training and prediction times.\n",
    "# Noise Reduction: PCA can help in removing noise and less relevant features, leading to cleaner and more robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baefb231-a5e1-4103-a91b-02ecc4826cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ca084e-47d4-493e-8a8a-45074f71d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction: Simplifies datasets with many features, improving efficiency.\n",
    "# Data Visualization: Converts high-dimensional data to 2D or 3D for easier visualization.\n",
    "# Noise Reduction: Filters out noise by focusing on significant principal components.\n",
    "# Feature Extraction: Generates new, meaningful features from the original data.\n",
    "# Anomaly Detection: Spots outliers by examining deviations in the principal component space.\n",
    "# Preprocessing for Machine Learning: Enhances model performance by reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415a1231-1161-4690-97b6-ceabcfe289eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb48993b-920c-4304-abce-4cd0e58a10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance: Measures how much the data varies along each principal component. In PCA, the principal components are chosen to \n",
    "# maximize variance, meaning they capture the directions in which the data varies the most.\n",
    "\n",
    "# Spread: Refers to the range or extent of data points along the principal components. A larger spread indicates a higher \n",
    "# variance along that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa7a61c-1a66-41d6-bb9c-a12c24ca375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a0389d-c9af-4ace-ba79-700f08c448cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Covariance Matrix: Calculate the covariance matrix of the data to understand how features vary together.\n",
    "\n",
    "# Calculate Eigenvalues and Eigenvectors: Find the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues\n",
    "# represent the amount of variance captured by each principal component, while the eigenvectors represent the directions\n",
    "# (principal components) of this variance.\n",
    "\n",
    "# Sort Components by Variance: Rank the principal components (eigenvectors) by their corresponding eigenvalues. Components \n",
    "# with higher eigenvalues capture more variance (spread) in the data.\n",
    "\n",
    "# Select Top Components: Choose the top principal components that capture the most variance. This reduces the dimensionality\n",
    "# of the data while retaining the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d780bd0-fe6c-422d-b151-dd239225a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260bf25d-c8cd-4fb6-9135-32d1a31f2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the covariance matrix to understand feature relationships.\n",
    "# Performing eigenvalue decomposition to find principal components.\n",
    "# Prioritizing principal components with higher eigenvalues, which capture more variance.\n",
    "# Reducing dimensionality by selecting top components with significant variance and discarding those with low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b19d0a-6722-48d7-b516-e6eaf1f9ec60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fcb44-aaff-464e-b1de-763adddeaecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c3e25-3c7b-4199-a72b-e081fa6e053c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
