{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95afa66-7644-4c36-9278-4447981b4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f1b149-0310-47b9-843b-5d4a14672659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contingency matrix, also known as a confusion matrix, is a table used to evaluate the performance of a classification \n",
    "# model. It provides a summary of the predictions against the actual true labels, helping you understand how well your\n",
    "# model is performing.\n",
    "\n",
    "# \t              Predicted Class 0\t        Predicted Class 1\n",
    "# Actual Class 0\tTP (True Positives)\tFP (False Positives)\n",
    "# Actual Class 1\tFN (False Negatives)\tTP (True Positives)\n",
    "\n",
    "# Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "# Precision: TP / (TP + FP)\n",
    "# Recall: TP / (TP + FN)\n",
    "# F1-score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "\n",
    "# Correct classifications: True Positives (TP) and True Negatives (TN)\n",
    "# Misclassifications: False Positives (FP) and False Negatives (FN)\n",
    "# Class imbalance: Identify if your model is biased towards a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04c4bf1-9d90-4c27-ba44-a1f999992484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "# certain situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7d88e5-89d6-4843-9d06-80136ce5b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pair confusion matrix evaluates how often pairs of classes are confused in a classification model, unlike a regular\n",
    "# confusion matrix that shows counts for individual classes. It provides detailed insights into specific class\n",
    "# pairs' misclassification rates, useful for understanding and improving model performance on pairs of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80351032-2874-464f-81ca-4f7f00e35eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "# used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5de23f-b7d2-4e04-97f0-8e1143f1eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An extrinsic measure in NLP evaluates a model's performance based on how well it performs on a specific task or\n",
    "# application outside of its training data. It involves testing the model in real-world scenarios or tasks to determine\n",
    "# its effectiveness.\n",
    "\n",
    "# For example, extrinsic measures for a language model might include:\n",
    "\n",
    "# Task Performance: Evaluating the model's ability to perform specific tasks like translation, summarization, or question\n",
    "# answering.\n",
    "# Application Integration: Assessing how well the model improves outcomes in applications like chatbots or search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3070388-c578-4f74-b9f8-d35c7cf7f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "# extrinsic measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e727a5c-a44a-4db9-8505-537827527653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# An intrinsic measure in machine learning evaluates a model's performance based on metrics that are directly related to \n",
    "# the model itself, independent of any specific application or task. These measures focus on the quality of the model's\n",
    "# predictions or the features of the model.\n",
    "\n",
    "# Examples of intrinsic measures include:\n",
    "\n",
    "# Accuracy: The proportion of correct predictions out of all predictions.\n",
    "# Precision and Recall: Metrics that measure the correctness of positive predictions and the ability to capture all relevant\n",
    "# instances.\n",
    "# F1 Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "# Log-Loss: Measures the performance of a classification model by quantifying the accuracy of the predicted probabilities.\n",
    "\n",
    "# Differences from extrinsic measures:\n",
    "# Focus: Intrinsic measures focus on the model's internal metrics, whereas extrinsic measures focus on the model's\n",
    "# performance in real-world applications or tasks.\n",
    "# Application: Intrinsic measures are used to assess the model during training and evaluation phases, while extrinsic\n",
    "# measures evaluate how well the model performs in practical, specific scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722826c2-d210-478a-949c-88b58a59d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "# strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82d50fe-c242-4f42-b98b-004395d1f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A confusion matrix is a tool used to evaluate a classification model by summarizing its performance with true positives,\n",
    "# true negatives, false positives, and false negatives. It helps identify how well the model is predicting each class and \n",
    "# highlights areas where the model is making errors. This allows for assessing strengths and weaknesses in the model’s \n",
    "# performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78516d87-8f20-4255-b3ca-e9a33077200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "# learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4182ea8-28de-4dc9-8de3-ce5d429c4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score: Measures how similar each data point is to its own cluster compared to other clusters. Values \n",
    "# range from -1 (poor) to 1 (good). A high value indicates well-separated and distinct clusters.\n",
    "\n",
    "# Davies-Bouldin Index: Evaluates the average similarity ratio of each cluster with its most similar cluster. Lower values\n",
    "# indicate better clustering, with fewer overlaps between clusters.\n",
    "\n",
    "# Within-Cluster Sum of Squares (WCSS): Measures the total variance within each cluster. Lower WCSS indicates that data \n",
    "# points within clusters are closer to each other, suggesting better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921006af-8282-424c-95ec-ee87ee900e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "# how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262d4b19-2610-4184-ae47-432c1fe8db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance: Accuracy doesn’t reflect performance on minority classes if there is a class imbalance.\n",
    "# Error Types: Accuracy doesn’t distinguish between different types of errors (e.g., false positives vs. false negatives).\n",
    "# Multi-Class Performance: Accuracy provides a single overall measure but doesn’t detail performance across multiple classes.\n",
    "\n",
    "# Addressing Limitations:\n",
    "#     Precision and Recall: Evaluate performance on individual classes.\n",
    "#     F1 Score: Combines precision and recall into a single metric.\n",
    "#     Confusion Matrix: Shows detailed performance across all classes, including true positives, false positives, true\n",
    "#     negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab9122-5606-44f8-a720-5f75f1426248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d91d48-6d31-4396-8460-5833fc058275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
