{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c92cca7-4130-42cd-956f-8dbac993ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e111686c-47f8-4242-82f6-05f6f10a27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse Training Sets: Training multiple trees on different random subsets of the data helps capture various patterns and\n",
    "# reduces overfitting to any single subset.\n",
    "\n",
    "# Aggregation: Combining the predictions of individual trees (by averaging or majority voting) smooths out errors and leads \n",
    "# to a more generalized model.\n",
    "\n",
    "# Variance Reduction: Averaging the predictions of multiple trees reduces the high variance associated with individual \n",
    "# trees, making the overall model more stable and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35ae9d0-d41c-4398-838a-4e80c40278e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc6cc7e-a058-4cfa-b799-1b26d9b5a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Using Different Types of Base Learners in Bagging:\n",
    "# Increased Diversity: Different base learners capture different aspects of the data, leading to greater diversity and \n",
    "# potentially better overall performance.\n",
    "# Improved Robustness: Combining diverse models can lead to more robust predictions and improved generalization by leveraging\n",
    "# the strengths of each base learner.\n",
    "# Reduced Bias: Using different base learners can help in reducing the bias of the ensemble if individual models have varying\n",
    "# strengths.\n",
    "\n",
    "# Disadvantages of Using Different Types of Base Learners in Bagging:\n",
    "# Complexity: Managing and combining different types of base learners can increase the complexity of the model and its\n",
    "# implementation.\n",
    "# Increased Computational Cost: Training and combining multiple diverse models can be computationally expensive and\n",
    "# time-consuming.\n",
    "# Difficulty in Interpretation: The resulting ensemble might be harder to interpret and understand compared to an \n",
    "# ensemble of similar base learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9188719a-f50f-47ce-8166-33659aac79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ccfc2a-418d-4b0f-bf10-ff55b8f3e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-Bias Models: Bagging primarily helps in reducing variance but cannot significantly reduce bias. The resulting ensemble\n",
    "# may still underperform if base learners are too simple.\n",
    "# High-Variance Models: Bagging effectively reduces variance and stabilizes predictions, making the ensemble more robust. \n",
    "# It helps counteract overfitting by combining predictions from multiple complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ef2589-4217-4cb1-ad9c-c359b645a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f1607d-a46d-4a2b-86ce-5a2b14911262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: Predictions from individual models are combined using majority voting to determine the final class.\n",
    "# Bagging reduces variance and improves accuracy by averaging out errors.\n",
    "\n",
    "# Regression: Predictions from individual models are averaged to provide the final estimate. Bagging reduces variance and\n",
    "# improves prediction accuracy by averaging out errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56181c4b-2e93-4c93-983c-5b4dd2ea4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92489a0-29c1-4b51-ae43-f81ba7a90de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In bagging, the ensemble size (number of base models) plays a crucial role in reducing variance and improving performance.\n",
    "# While increasing the number of models generally enhances stability and accuracy, the benefits may diminish after a certain\n",
    "# point. Typically, 50 to 100 models are used, but the optimal size depends on the problem, dataset, and available\n",
    "# computational resourc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa48bdd8-4f9a-4c5f-8e3f-295fa3ab7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9150c0-f19f-45ec-94cb-7914bf7dd0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
