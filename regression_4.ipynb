{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fede8365-173f-4dd4-9fdd-445240c86678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf8011d-8555-43ee-8a7b-f1a9dd11e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression is a regularized regression technique that uses an L1 penalty to shrink some coefficients \n",
    "# to zero, effectively performing feature selection. It differs from OLS by including regularization and from\n",
    "# Ridge by its ability to eliminate irrelevant features, making it especially useful\n",
    "# in high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a205021-f9b1-476e-ae88-a050f134571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b222d4-4561-4d28-b5c2-72b399d79e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression's main advantage in feature selection is its ability to automatically eliminate irrelevant features\n",
    "# by shrinking their coefficients to zero, leading to a simpler, more interpretable model with improved generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e395c62b-1934-4ae8-a30c-2fc1926fb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae4aa40-b6c4-425c-8c04-fe0dbca8265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Lasso Regression, coefficients indicate the direction and strength of the relationship between features and the target\n",
    "# variable. A zero coefficient means that the feature is not important for the model. The regularization effect causes \n",
    "# shrinkage, which helps in reducing overfitting but may alter the scale of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c51007-16a6-45c5-baa9-53625d235fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "# model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2040cee2-e356-462d-8452-e62cfd076650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Lasso Regression, the key tuning parameter is 位 (lambda), which controls the degree of regularization. Adjusting 位 \n",
    "# affects the number of features selected and the trade-off between bias and variance. High 位 values lead to stronger \n",
    "# regularization and simpler models, while low 位 values allow more features to be included, increasing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec4360b-e49f-417f-882e-3e1d7586c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9735d8-f8ab-4cf2-971b-1e74b820e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While Lasso Regression is inherently a linear model, it can be adapted for non-linear problems by transforming the features \n",
    "# (e.g., using polynomial features, kernels, or other non-linear basis functions) before applying the regression. \n",
    "# The regularization in Lasso will then help select the most relevant non-linear features for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810aca67-31d0-4b8d-bcbc-d09ba4d5dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b1c2d84-22e3-4bad-97e4-7d8efa2df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression shrinks coefficients but retains all features, making it suitable for situations with multicollinearity \n",
    "# or when all features are believed to contribute to the outcome.\n",
    "\n",
    "# Lasso Regression can reduce some coefficients to zero, effectively selecting a subset of features, making it useful when\n",
    "# you need to simplify the model and identify the most significant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6752f443-0597-4807-bbb2-e3f7505436ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b30a0-1f40-4247-aa1e-1f72ea1fab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regression can handle multicollinearity by selecting one feature from a group of correlated features and shrinking the others to zero, thus reducing redundancy and simplifying the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
