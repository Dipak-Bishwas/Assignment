{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc21df99-9711-4433-b07b-b7bc10d6e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15d0a8c-7c18-46cc-851c-632c52a827a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression focuses solely on minimizing prediction errors without regularization.\n",
    "\n",
    "# Ridge Regression adds a penalty to the size of the coefficients to address multicollinearity and overfitting, leading to \n",
    "# more stable and generalized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a5717e-0e46-4d41-8fe5-ab51fb4cc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bded08-218c-4dc7-90df-41145bb19e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression assumes linearity, addresses multicollinearity, and is robust to deviations\n",
    "# fromnormality and independence of errors. It also helps improve model stability and prevent \n",
    "# overfitting by shrinking coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c411425-5ddd-411d-ad2a-e5b530634f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce85efc3-7889-4ab1-8118-b6252cb534cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select the value of the tuning parameter λ in Ridge Regression, use cross-validation to assess performance across\n",
    "# different λ values, apply grid search for systematic evaluation, leverage regularization path algorithms, and consider\n",
    "# performance metrics to ensure the best model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43328b37-ea30-4828-87c1-4e22807ce7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66919a13-47b5-471e-bd6f-8020c53ed8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression does not perform explicit feature selection as it keeps all features but shrinks their coefficients.\n",
    "# For actual feature selection, methods like Lasso Regression are more suitable. Ridge can still be useful for reducing the\n",
    "# influence of less important features without removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae41371-a92b-48e7-af97-2e18856eb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba64d0f-871c-4501-8e72-851ac9bb8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression performs well in the presence of multicollinearity by stabilizing coefficient estimates and \n",
    "# improving model generalization. It mitigates the issues caused by highly correlated predictors but does not eliminate \n",
    "# any features from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af317ac-247e-4dec-a8b5-15c95af050d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c624186c-90a3-4815-b6aa-5f1df0c032fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression can handle both categorical and continuous independent variables, but categorical variables\n",
    "# must be encoded into a numerical format first. Continuous variables are used directly, while categorical variables\n",
    "# require preprocessing to be included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78d3481-3656-4b4e-b4a4-f7362be4f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab57dbd7-4440-4d0e-a6ca-46eccdbd1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Ridge Regression, coefficients represent the impact of each predictor on the target variable, with their\n",
    "# magnitude and direction indicating influence and relationship. Coefficients are shrunk toward zero due to\n",
    "# regularization, making them more stable and less affected by multicollinearity compared to OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678159dc-54d8-4c5e-b10a-f65c43f0b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f41627-4daf-4843-957c-df145fdccf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
